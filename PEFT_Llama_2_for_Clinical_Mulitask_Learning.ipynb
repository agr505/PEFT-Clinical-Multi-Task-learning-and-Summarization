{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLXwJqbjtPho",
        "outputId": "4731cd16-a057-48cd-96ac-5f1de940e8d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 scipy utils"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Python Packages"
      ],
      "metadata": {
        "id": "glfOvC_bGJ0y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAMzy_0FtaUZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import *\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel, get_peft_model,  prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwGZBBrE9Uwb",
        "outputId": "87f9a743-57d0-4f3c-d485-7485dcbe7538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n",
            "done\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "access_token_read = \"\"\n",
        "\n",
        "login(token = access_token_read)\n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing Utils"
      ],
      "metadata": {
        "id": "T_HEVGEu_g19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "\n",
        "import transformers\n",
        "from transformers import Trainer\n",
        "\n",
        "\n",
        "def modify_special_tokens(tokenizer):\n",
        "    tokenizer.add_special_tokens(\n",
        "        {\n",
        "            \"pad_token\": \"<s>\",\n",
        "            \"eos_token\": \"</s>\",\n",
        "            \"bos_token\": \"<s>\",\n",
        "            \"unk_token\": \"<unk>\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "    tokenizer.eos_token_id = 2\n",
        "    tokenizer.bos_token_id = 1\n",
        "    tokenizer.unk_token_id = 0\n",
        "    tokenizer.pad_token_id = 1\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataArguments:\n",
        "    data_path: str = field(\n",
        "        default=None, metadata={\"help\": \"Path to the training data.\"}\n",
        "    )\n",
        "    lazy_preprocess: bool = False\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainingArguments(transformers.TrainingArguments):\n",
        "    cache_dir: Optional[str] = field(default=None)\n",
        "    optim: str = field(default=\"adamw_torch\")\n",
        "    model_max_length: int = field(\n",
        "        default=2048,\n",
        "        metadata={\n",
        "            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n",
        "        },\n",
        "    )\n",
        "    remove_unused_columns: bool = field(\n",
        "        default=False,\n",
        "    )\n",
        "    dataloader_num_workers: int = field(\n",
        "        default=16,\n",
        "    )\n",
        "\n",
        "\n",
        "PROMPT_DICT = {\n",
        "    \"ours\": \"\"\"You are an intelligent clinical languge model.\n",
        "Below is a snippet of patient's discharge summary and a following instruction from healthcare professional.\n",
        "Write a response that appropriately completes the instruction.\n",
        "The response should provide the accurate answer to the instruction, while being concise.\n",
        "\n",
        "[Discharge Summary Begin]\n",
        "{note}\n",
        "[Discharge Summary End]\n",
        "\n",
        "[Instruction Begin]\n",
        "{question}\n",
        "[Instruction End]\n",
        "\"\"\",\n",
        "    \"alpaca\": (\n",
        "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction:\\n{question}\\n\\n### Input:\\n{note}\\n\\n### Response:\"\n",
        "    ),\n",
        "    \"medalpaca\": (\n",
        "        \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\"\n",
        "        \"\\n\\n### Instruction:\\n{question}\\n\\n### Input:\\n{note}\\n\\n### Response:\\n\"\n",
        "    ),\n",
        "    \"chat\": \"\"\"\n",
        "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: [The start of the Discharge Summary]\n",
        "{note}\n",
        "[The end of the Discharge Summary]\n",
        "{question} ASSISTANT:\n",
        "\"\"\",\n",
        "}\n",
        "\n",
        "\n",
        "def get_prompt(model_name):\n",
        "    if model_name in [\"decapoda-research/llama-7b-hf\", \"chaoyi-wu/PMC_LLAMA_7B\"]:\n",
        "        print(\"Using Ours+Response Prompt\")\n",
        "        return PROMPT_DICT[\"ours\"] + \"\\nResponse: \"\n",
        "    # chatdoctor, alpaca ,medalpaca\n",
        "    elif model_name in [\n",
        "        \"chavinlo/alpaca-native\",\n",
        "        \"zl111/ChatDoctor\",\n",
        "    ]:\n",
        "        print(\"Using Alpaca Prompt\")\n",
        "        return PROMPT_DICT[\"alpaca\"]\n",
        "    elif model_name == \"medalpaca/medalpaca-7b\":\n",
        "        print(\"Using MedAlpaca Prompt\")\n",
        "        return PROMPT_DICT[\"medalpaca\"]\n",
        "    elif \"vicuna\" in model_name or \"clinical-camel\" in model_name:\n",
        "        print(\"Using Vicuna Prompt\")\n",
        "        return PROMPT_DICT[\"chat\"]\n",
        "    else:\n",
        "        print(\"Using Our Prompt\")\n",
        "        return PROMPT_DICT[\"ours\"]"
      ],
      "metadata": {
        "id": "eKl0juxk_dHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preprocessing and Collation"
      ],
      "metadata": {
        "id": "KtcSgs9x_6J2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import io\n",
        "import json\n",
        "import logging\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Sequence\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from utils import *\n",
        "\n",
        "#if \"A100\" in torch.cuda.get_device_name():\n",
        "#    from llama_flash_attn_monkey_patch import replace_llama_attn_with_flash_attn\n",
        "\n",
        "#    replace_llama_attn_with_flash_attn()\n",
        "\n",
        "\n",
        "IGNORE_INDEX = -100\n",
        "\n",
        "PROMPT = \"\"\"You are an intelligent clinical languge model.\n",
        "Below is a snippet of patient's discharge summary and a following instruction from healthcare professional.\n",
        "Write a response that appropriately completes the instruction.\n",
        "The response should provide the accurate answer to the instruction, while being concise.\n",
        "\n",
        "[Discharge Summary Begin]\n",
        "{note}\n",
        "[Discharge Summary End]\n",
        "\n",
        "[Instruction Begin]\n",
        "{question}\n",
        "[Instruction End]\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def jload(f, mode=\"r\"):\n",
        "    \"\"\"Load a .json file into a dictionary.\"\"\"\n",
        "    if not isinstance(f, io.IOBase):\n",
        "        f = open(f, mode=mode)\n",
        "    jdict = json.load(f)\n",
        "    f.close()\n",
        "    return jdict\n",
        "\n",
        "\n",
        "def _tokenize_fn(\n",
        "    strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer\n",
        ") -> Dict:\n",
        "    \"\"\"Tokenize a list of strings.\"\"\"\n",
        "    tokenized_list = [\n",
        "        tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"longest\",\n",
        "            max_length= 2048,\n",
        "            truncation=True,\n",
        "        )\n",
        "        for text in strings\n",
        "    ]\n",
        "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "    input_ids_lens = labels_lens = [\n",
        "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item()\n",
        "        for tokenized in tokenized_list\n",
        "    ]\n",
        "    return dict(\n",
        "        input_ids=input_ids,\n",
        "        labels=labels,\n",
        "        input_ids_lens=input_ids_lens,\n",
        "        labels_lens=labels_lens,\n",
        "    )\n",
        "\n",
        "\n",
        "def preprocess(\n",
        "    sources: Sequence[str],\n",
        "    targets: Sequence[str],\n",
        "    tokenizer: transformers.PreTrainedTokenizer,\n",
        ") -> Dict:\n",
        "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
        "    examples = [s + t for s, t in zip(sources, targets)]\n",
        "    examples_tokenized, sources_tokenized = [\n",
        "        _tokenize_fn(strings, tokenizer) for strings in (examples, sources)\n",
        "    ]\n",
        "    input_ids = examples_tokenized[\"input_ids\"]\n",
        "    labels = copy.deepcopy(input_ids)\n",
        "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
        "        label[:source_len] = IGNORE_INDEX\n",
        "    return dict(input_ids=input_ids, labels=labels)\n",
        "\n",
        "\n",
        "class SupervisedDataset(Dataset):\n",
        "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer):\n",
        "        super(SupervisedDataset, self).__init__()\n",
        "        logging.warning(\"Loading data...\")\n",
        "        list_data_dict = jload(data_path)\n",
        "\n",
        "        # Preprocess start/end \\n in the data\n",
        "        for i in range(len(list_data_dict)):\n",
        "            for k, v in list_data_dict[i].items():\n",
        "                if isinstance(v, str):\n",
        "                    list_data_dict[i][k] = v.strip(\"\\n\")\n",
        "\n",
        "        logging.warning(\"Formatting inputs...\")\n",
        "\n",
        "        sources = [PROMPT.format_map(example) for example in list_data_dict]\n",
        "        targets = [\n",
        "            f\"{example['answer']}{tokenizer.eos_token}\" for example in list_data_dict\n",
        "        ]\n",
        "\n",
        "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
        "        data_dict = preprocess(sources, targets, tokenizer)\n",
        "\n",
        "        self.input_ids = data_dict[\"input_ids\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForSupervisedDataset(object):\n",
        "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, labels = tuple(\n",
        "            [instance[key] for instance in instances] for key in (\"input_ids\", \"labels\")\n",
        "        )\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(\n",
        "            labels, batch_first=True, padding_value=IGNORE_INDEX\n",
        "        )\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        )\n",
        "\n",
        "\n",
        "def make_supervised_data_module(\n",
        "    tokenizer: transformers.PreTrainedTokenizer, data_args\n",
        ") -> Dict:\n",
        "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
        "    train_dataset = SupervisedDataset(\n",
        "        tokenizer=tokenizer, data_path=\"train_set.json\"\n",
        "    )\n",
        "    print('Finished Training set processing')\n",
        "    validation_dataset = SupervisedDataset(\n",
        "        tokenizer=tokenizer, data_path=\"validation_set.json\"\n",
        "    )\n",
        "    print('Finished Test set processing')\n",
        "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
        "    return dict(\n",
        "        train_dataset=train_dataset, eval_dataset=validation_dataset, data_collator=data_collator\n",
        "    )"
      ],
      "metadata": {
        "id": "lOGjsEEl_u0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Operate on Dataset"
      ],
      "metadata": {
        "id": "FteLMBM_A4VL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xft9IYxf-Hwm"
      },
      "outputs": [],
      "source": [
        "# Load dataset (you can process it here)\n",
        "# The instruction dataset to use\n",
        "dataset_name = \"starmpcc/Asclepius-Synthetic-Clinical-Notes\"\n",
        "\n",
        "dataset = load_dataset(dataset_name)\n",
        "\n",
        "train_test = dataset['train'].train_test_split(test_size=0.2)\n",
        "\n",
        "\n",
        "#dataset_processed = DatasetDict({\n",
        "#    'train': train_test['train'],\n",
        "#    'test': train_test['test']\n",
        "#  )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to reduce dataset\n",
        "def reduce_dataset(ds, num_rows):\n",
        "    num_rows_to_keep = min(num_rows, len(ds))\n",
        "    indices_to_keep = list(range(num_rows_to_keep))\n",
        "    return ds.select(indices_to_keep)\n",
        "\n",
        "# Define the number of rows you want to keep for each split\n",
        "num_train_rows = 5000\n",
        "num_test_rows = 1000  # Adjust as necessary\n",
        "\n",
        "# Create a new DatasetDict with reduced splits\n",
        "dataset_reduced = DatasetDict({\n",
        "    'train': reduce_dataset(train_test['train'], num_train_rows),\n",
        "    'test': reduce_dataset(train_test['test'], num_test_rows)\n",
        "})\n"
      ],
      "metadata": {
        "id": "7hVI_eyichA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LY7js21BQPy",
        "outputId": "1347000c-4801-4fa6-e99e-acb029bd8db7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the training partition: 5000\n",
            "Number of rows in the training partition: 1000\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of rows in the training partition: {len(dataset_reduced['train'])}\")\n",
        "#print(f\"Number of rows in the training partition: {len(dataset_processed['valid'])}\")\n",
        "print(f\"Number of rows in the training partition: {len(dataset_reduced['test'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1):\n",
        "    example = train_test['train'][i]\n",
        "    for key, value in example.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPj59q7sG2EV",
        "outputId": "4a111e72-84ec-42ad-8da2-2920b0ff786b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "patient_id: 91176\n",
            "note: Discharge Summary:\n",
            "\n",
            "Admission Date: May 2006\n",
            "\n",
            "Discharge Date: N/A\n",
            "\n",
            "Primary Diagnosis: CD grade Шb according to the Marsh classification, portal hypertension, splenomegaly, and non-specific chronic hepatitis.\n",
            "\n",
            "Hospital Course:\n",
            "\n",
            "Patient presented with malaise, weight loss, and edema of the lower limbs. Physical examination showed cachexia, anemia, thrombocytopenia, and leucopenia. Abdominal sonography revealed splenomegaly with large amounts of ascites. Duplex doppler ultrasonography confirmed portal hypertension. CT scan showed no evidence of vascular obstruction in splenoportal axis. Eosophagogastroduodenoscopy showed esophageal varices, gastric fundal varices, and portal hypertensive gastropathy. Biopsy results showed CD grade Шb and serologic studies revealed positive anti tissue transglutaminase (tTG) and anti endomysial antibody (EMA). A gluten-free diet was advised. Patient did not return for follow-up visits until 1 year later due to lack of compliance with GFD. Abdominal CT scan showed splenomegaly, thickening of the small intestine, and multiple soft tissue densities in the mesentery. Push enteroscopy revealed multiple jejunal ulcers and a mass lesion, which was biopsied. Seven days later the patient developed an acute abdomen and underwent an emergency laparotomy. Perforations of the small intestine at two sites were seen. Biopsies were compatible with ulcerative jejunitis and intestinal T-cell lymphoma.\n",
            "\n",
            "Follow-Up Care:\n",
            "\n",
            "The patient was referred to an oncologist but unfortunately died 2 weeks later.\n",
            "question: What were the diagnostic and biopsy results that led to the patient's diagnosis of ulcerative jejunitis and intestinal T-cell lymphoma, as described in the discharge summary?\n",
            "answer: The biopsy results that led to the patient's diagnosis of ulcerative jejunitis and intestinal T-cell lymphoma were the presence of ulcerative jejunitis on biopsy and intestinal T-cell lymphoma on subsequent laparotomy biopsy.\n",
            "task: Question Answering\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1):\n",
        "    example = train_test['test'][i]\n",
        "    for key, value in example.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiFtGti9HeIu",
        "outputId": "7d3d8fcc-5c3c-4881-a65d-848ebbaa13c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "patient_id: 76205\n",
            "note: Hospital Course Summary\n",
            "\n",
            "Patient ID: 12345\n",
            "Date of admission: October 2009\n",
            "Date of discharge: February 2010\n",
            "\n",
            "Diagnosis: Primary CNS lymphoma (PCNSL)\n",
            "\n",
            "Hospital course:\n",
            "The patient presented with complaints of right sided hemiparesis. An MRI of the brain showed a large mass in the left frontal area and a biopsy showed that the tumor consisted of a diffuse proliferation of large lymphoid cells. A CD20 immunostain was strongly positive in the tumor cells. A ki-67 immunostain showed a proliferation index of approximately 90%. A bcl-2 immunostain was positive, whereas immunostains for bcl-1, CD5, and CD10 were negative. A FISH study was negative for MYC translocation. Workup for systemic disease including bone marrow biopsy, CT scans, and a lumbar puncture for CSF examination was all negative.\n",
            "\n",
            "The patient was diagnosed with PCNSL and was treated with 8 cycles of high dose methotrexate, with significant improvement in his symptoms. He was given adjuvant radiation after completion of his chemotherapy and acquired complete remission. A posttreatment MRI brain showed no recurrent neoplasm. He stayed in remission until August 2012.\n",
            "\n",
            "Date of admission: August 2012\n",
            "Date of discharge: September 2013\n",
            "\n",
            "Diagnosis: Recurrent CNS lymphoma\n",
            "\n",
            "Hospital course:\n",
            "The patient presented with a palpable 1.5 cm nodule in the lumbosacral region, the site of a previous lumbar puncture. A biopsy of the lesion showed a diffuse large B cell lymphoma, consistent with recurrence of the original CNS lymphoma. He underwent local excision followed by chemotherapy in the form of 4 cycles of the R-CHOP regimen. Postchemotherapy consolidation radiotherapy was directed at the lumbosacral spine region. The patient acquired complete remission, with the area showing no palpable mass, hypopigmentation, or wound dehiscence. At the last visit, the patient continued to be in remission.\n",
            "question: What coreferential expressions were resolved in the given discharge summary related to the patient's diagnosis of recurrent CNS lymphoma?\n",
            "answer: The coreferential expressions that were resolved in the given discharge summary related to the patient's diagnosis of recurrent CNS lymphoma are \"the original CNS lymphoma\" and \"the lesion,\" which both refer to the patient's recurrent CNS lymphoma.\n",
            "task: Coreference Resolution\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "data_list = dataset_reduced['train'].to_pandas().to_dict(orient=\"records\")\n",
        "with open('train_set.json', 'w') as f:\n",
        "  json.dump(data_list, f, indent=4)\n",
        "\n",
        "data_list = dataset_reduced['test'].to_pandas().to_dict(orient=\"records\")\n",
        "with open('validation_set.json', 'w') as f:\n",
        "  json.dump(data_list, f, indent=4)\n",
        "\n",
        "#train_test['train'].to_json('train_set.json', orient=\"records\")#, indent=4)\n",
        "#train_test['test'].to_json('validation_set.json', orient=\"records\")#, indent=4)\n",
        "\n"
      ],
      "metadata": {
        "id": "27gNnHCHHliy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization Settings"
      ],
      "metadata": {
        "id": "ZXVs4cSi9ZSX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nrorjho_-Ott"
      },
      "outputs": [],
      "source": [
        "\n",
        "################################################################################\n",
        "# bitsandbytes parameters\n",
        "################################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4), Set to 4-bit NormalFloat to enable QLoRA\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = True\n",
        "\n",
        "\n",
        "# Load tokenizer and model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Model and Tokenizer"
      ],
      "metadata": {
        "id": "Z7fUJtc79Qpa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1JaQtZ6B-ri"
      },
      "outputs": [],
      "source": [
        "# The model that you want to train from the Hugging Face hub\n",
        "#model_name = \"NousResearch/Llama-2-7b-hf\"\n",
        "#Space considerations\n",
        "model_name =\"meta-llama/Llama-2-7b-hf\"\n",
        "\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = \"llama-2-7b-clinical-summarization\"\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "print(\"Loaded Model\")\n",
        "# Load LLaMA tokenizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name =\"meta-llama/Llama-2-7b-hf\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"
      ],
      "metadata": {
        "id": "6hn-pznFSDsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Dataset Module"
      ],
      "metadata": {
        "id": "kyYRsJQpISZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = modify_special_tokens(tokenizer)\n",
        "\n",
        "data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nhdZ28kIOsn",
        "outputId": "c8e0151e-6c00-4f8a-b496-9b3bc8bc04d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Loading data...\n",
            "WARNING:root:Formatting inputs...\n",
            "WARNING:root:Tokenizing inputs... This may take some time...\n",
            "WARNING:root:Loading data...\n",
            "WARNING:root:Formatting inputs...\n",
            "WARNING:root:Tokenizing inputs... This may take some time...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Training set processing\n",
            "Finished Test set processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FT Memory Usage and Lora"
      ],
      "metadata": {
        "id": "o_RUvRKX9h5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine Tuning Memory Considerations, Lora Hyperparameters**\n",
        "\n",
        " Naively fine-tuning a 7B model requires about 7 x 4 x 4 = 112 GB of VRAM (considering Parameters, Gradients, and AdamW Optimizer states)\n",
        "\n",
        "**Lora comparision**:\n",
        "\n",
        "LLama 2 7 billion Hidden Dim (d_model) = 4096  >> Lora Rank = {16, 32, 128, 256}\n",
        "\n",
        "LLama 2 13 billion Hidden Dim (d_model) = 5120  >> Lora Rank = {16, 32, 128, 256}\n",
        "\n",
        "**QLora** offers 33% memory savings at the cost of a 39% increase in runtime."
      ],
      "metadata": {
        "id": "Q5xf4dQXdx2F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJXpOgBFuSrc"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# QLoRA parameters\n",
        "################################################################################\n",
        "\n",
        "# LoRA rank\n",
        "lora_r = 64\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 64\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.1\n",
        "\n",
        "\n",
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    target_modules=[\"query_key_value\", \"gate_proj\", \"down_proj\", \"up_proj\", \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uY5qFevB9Uwd"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('data_module.pickle', 'wb') as handle:\n",
        "    pickle.dump(data_module, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Hyperparameters"
      ],
      "metadata": {
        "id": "-UUhRf5q-SHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# TrainingArguments parameters\n",
        "################################################################################\n",
        "\n",
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 1\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 4\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 4\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning rate schedule\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = -1\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 0\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 25\n",
        "\n",
        "################################################################################\n",
        "# SFT parameters\n",
        "################################################################################\n",
        "\n",
        "# Maximum sequence length to use\n",
        "max_seq_length = 2048\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False"
      ],
      "metadata": {
        "id": "AddQXE28-Lo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conduct Training"
      ],
      "metadata": {
        "id": "V3lrmqz4-ajl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQfIbxTJB4vu"
      },
      "outputs": [],
      "source": [
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=50,\n",
        "    learning_rate=learning_rate,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    **data_module,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Save trained model\n",
        "trainer.model.save_pretrained(new_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crj9svNe4hU5"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir results/runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkQCviG0Zta-",
        "outputId": "e7c4ab10-4039-4490-b7f0-6ea118bdd709"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19965"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Empty VRAM\n",
        "del model\n",
        "del pipe\n",
        "del trainer\n",
        "import gc\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-xPb-_qB0dz"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login\n",
        "\n",
        "model.push_to_hub(new_model, use_temp_dir=False)\n",
        "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_iVjCJU9Uwe",
        "outputId": "0ead5690-cb42-4677-959c-ef59d6a65b0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Collecting tensorboard\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting absl-py>=0.4 (from tensorboard)\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting grpcio>=1.48.2 (from tensorboard)\n",
            "  Downloading grpcio-1.64.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting markdown>=2.6.8 (from tensorboard)\n",
            "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from tensorboard) (1.26.3)\n",
            "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard)\n",
            "  Downloading protobuf-5.27.0-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from tensorboard) (68.2.2)\n",
            "Requirement already satisfied: six>1.9 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from tensorboard) (1.16.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
            "  Downloading werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
            "Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.64.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.27.0-cp38-abi3-manylinux2014_x86_64.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.2/309.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: werkzeug, tensorboard-data-server, protobuf, markdown, grpcio, absl-py, tensorboard\n",
            "Successfully installed absl-py-2.1.0 grpcio-1.64.0 markdown-3.6 protobuf-5.27.0 tensorboard-2.16.2 tensorboard-data-server-0.7.2 werkzeug-3.0.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install tensorboard"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}